- def: "abs(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  lower_fn: BuildAbs
  generics: {T: MachinaNumeric}

- def: "acos(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  lower_fn: xla::Acos
  generics: {T: MachinaNumeric}

- def: "acosh(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  lower_fn: xla::Acosh
  generics: {T: FloatingPoint & MachinaScalar}

- def: "add(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<T>"
  lower_fn: LowerBinaryOp<xla::Add>
  codira_name: addV2
  generics: {T: MachinaNumeric}

- def: "all(_ input: Tensor<Bool>, dims: [Int64], keep_reduced_dimensions: Bool) -> Tensor<Bool>"
  extras: ["canonicalize dims input"]
  lower_fn: BuildAll

- def: "any(_ input: Tensor<Bool>, dims: [Int64], keep_reduced_dimensions: Bool) -> Tensor<Bool>"
  extras: ["canonicalize dims input"]
  lower_fn: BuildAny

- def: "argmax(_ input: Tensor<T>, dim: Int64, keepdim: Bool) -> Tensor<Int64>"
  extras: ["canonicalize dim input"]
  lower_fn: BuildArgMax
  codira_name: argMax
  generics: {T: MachinaNumeric}
  result_dtype: Long

- def: "argmin(_ input: Tensor<T>, dim: Int64, keepdim: Bool) -> Tensor<Int64>"
  extras: ["canonicalize dim input"]
  lower_fn: BuildArgMin
  codira_name: argMin
  generics: {T: MachinaNumeric}
  result_dtype: Long

- def: "asin(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  lower_fn: xla::Asin
  generics: {T: MachinaNumeric}

- def: "asinh(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  lower_fn: xla::Asinh
  generics: {T: FloatingPoint & MachinaScalar}

- def: "atan(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  lower_fn: xla::Atan
  generics: {T: MachinaNumeric}

- def: "atanh(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  lower_fn: xla::Atanh
  generics: {T: FloatingPoint & MachinaScalar}

- def: "broadcast_tensors(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> (Tensor<T>, Tensor<T>)"
  lower_fn: LowerBroadcastTensors
  generics: {T: MachinaScalar}

- def: "cat(_ input: [Tensor<T>], dim: Int64) -> Tensor<T>"
  extras: ["canonicalize dim input CanonicalizeCat"]
  codira_name: concat
  generics: {T: MachinaScalar}
  lower_fn: BuildCat

- def: "ceil(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  generics: {T: FloatingPoint & MachinaScalar}
  lower_fn: xla::Ceil

- def: "clamp(t: Tensor<T>, clipValueMin: Tensor<T>, clipValueMax: Tensor<T>) -> Tensor<T>"
  codira_name: clipByValue
  generics: {T: MachinaNumeric}
  shape_fn: t
  lower_fn: LowerClamp

- def: "constant_pad_nd(_ input: Tensor<T>, _ pad: [Int64], _ value: AnyScalar) -> Tensor<T>"
  extras: ["canonicalize pad input CanonicalizePad"]
  generics: {T: MachinaScalar}
  lower_fn: LowerPad
  protection: internal

- def: "cos(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  generics: {T: FloatingPoint & MachinaScalar}
  lower_fn: xla::Cos

- def: "cosh(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  generics: {T: FloatingPoint & MachinaScalar}
  lower_fn: xla::Cosh

- def: "cumprod(_ input: Tensor<T>, dim: Int64, exclusive: Bool, reverse: Bool) -> Tensor<T>"
  extras: ["canonicalize dim input"]
  generics: {T: MachinaNumeric}
  shape_fn: input
  lower_fn: LowerCumProd

- def: "cumsum(_ input: Tensor<T>, dim: Int64, exclusive: Bool, reverse: Bool) -> Tensor<T>"
  extras: ["canonicalize dim input"]
  generics: {T: MachinaNumeric}
  shape_fn: input
  lower_fn: LowerCumSum

- def: "diagonal_value(_ input: Tensor<T>, offset: Int64, dim1: Int64, dim2: Int64) -> Tensor<T>"
  extras: ["canonicalize dim1 input", "canonicalize dim2 input"]
  generics: {T: MachinaNumeric}
  x10_enum: at::aten::diagonal
  lower_fn: BuildDiagonal

- def: "div(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<T>"
  generics: {T: MachinaNumeric}
  lower_fn: LowerBinaryOp<xla::Div>

- def: "dynamic_slice(_ base: Tensor<T>, _ start_indices: [Tensor<Int32>], _ slice_shapes: [Int64]) -> Tensor<T>"
  x10_enum: at::aten::xla_dynamic_slice
  codira_name: dynamicSlice
  generics: {T: MachinaNumeric}
  lower_fn: xla::DynamicSlice

- def: "dynamic_update_slice(_ base: Tensor<T>, _ update: Tensor<T>, _ start_indices: [Tensor<Int32>]) -> Tensor<T>"
  x10_enum: at::aten::xla_dynamic_update_slice
  codira_name: dynamicUpdateSlice
  generics: {T: MachinaNumeric}
  lower_fn: xla::DynamicUpdateSlice

- def: "eq(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<Bool>"
  lower_fn: LowerBinaryOp<xla::Eq>
  generics: {T: MachinaScalar}
  result_dtype: Bool

- def: "exp(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  generics: {T: FloatingPoint & MachinaScalar}
  lower_fn: xla::Exp

- def: "expand(_ input: Tensor<T>, dims: [Int64]) -> Tensor<T>"
  extras: ["canonicalize dims input CanonicalizeExpand"]
  generics: {T: MachinaScalar}
  codira_name: broadcastTo
  lower_fn: BuildExpand

- def: "expm1(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  generics: {T: FloatingPoint & MachinaScalar}
  lower_fn: xla::Expm1

- def: "flip(_ input: Tensor<T>, dims: [Int64]) -> Tensor<T>"
  extras: ["canonicalize dims input CanonicalizeFlip"]
  generics: {T: MachinaScalar}
  shape_fn: input
  lower_fn: xla::Rev

- def: "floor(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  generics: {T: FloatingPoint & MachinaScalar}
  lower_fn: xla::Floor

- def: "gather(_ input: Tensor<T>, indices: Tensor<Tindices>, start_dim: Int64) -> Tensor<T>"
  x10_enum: at::aten::index
  generics: {T: MachinaScalar, Tindices: MachinaIndex}
  lower_fn: CreateIndex

- def: "ge(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<Bool>"
  codira_name: greaterEqual
  generics: {T: MachinaNumeric}
  lower_fn: LowerBinaryOp<xla::Ge>
  result_dtype: Bool

- def: "gt(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<Bool>"
  codira_name: greater
  generics: {T: MachinaNumeric}
  lower_fn: LowerBinaryOp<xla::Gt>
  result_dtype: Bool

- def: "is_finite(_ input: Tensor<T>) -> Tensor<Bool>"
  x10_enum: at::aten::xla_is_finite
  codira_name: isFinite
  lower_fn: xla::IsFinite
  shape_fn: input
  generics: {T: FloatingPoint & MachinaScalar}
  result_dtype: Bool

- def: "is_inf(_ input: Tensor<T>) -> Tensor<Bool>"
  x10_enum: at::aten::xla_is_inf
  codira_name: isInf
  generics: {T: FloatingPoint & MachinaScalar}
  lower_fn: xla::IsInf
  shape_fn: input
  result_dtype: Bool

- def: "is_nan(_ input: Tensor<T>) -> Tensor<Bool>"
  x10_enum: at::aten::xla_is_nan
  shape_fn: input
  generics: {T: FloatingPoint & MachinaScalar}
  codira_name: isNan
  lower_fn: xla::IsNan
  result_dtype: Bool

- def: "le(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<Bool>"
  generics: {T: MachinaNumeric}
  codira_name: lessEqual
  lower_fn: LowerBinaryOp<xla::Le>
  result_dtype: Bool

- def: "log(_ input: Tensor<T>) -> Tensor<T>"
  generics: {T: FloatingPoint & MachinaScalar}
  shape_fn: input
  lower_fn: xla::Log

- def: "log1p(_ input: Tensor<T>) -> Tensor<T>"
  generics: {T: FloatingPoint & MachinaScalar}
  shape_fn: input
  lower_fn: xla::Log1p

- def: "log_softmax(_ input: Tensor<T>, dim: Int64) -> Tensor<T>"
  extras: ["canonicalize dim input"]
  codira_name: logSoftmax
  generics: {T: FloatingPoint & MachinaScalar}
  lower_fn: BuildLogSoftmax
  shape_fn: input

- def: "log_softmax_backward(gradOutput: Tensor<T>, output: Tensor<T>, dim: Int64) -> Tensor<T>"
  extras: ["canonicalize dim gradOutput"]
  x10_enum: at::aten::_log_softmax_backward_data
  generics: {T: FloatingPoint & MachinaScalar}
  codira_name: logSoftmaxBackward
  shape_fn: gradOutput
  lower_fn: BuildLogSoftmaxGrad

- def: "logicalAnd(_ lhs: Tensor<Bool>, _ rhs: Tensor<Bool>) -> Tensor<Bool>"
  x10_enum: at::aten::logical_and
  lower_fn: LowerBinaryOp<xla::And>

- def: "logical_cast(_ input: Tensor<Srct>, destType: ScalarType) -> Tensor<Dstt>"
  x10_enum: xla_symbols::cast
  generics: {Srct: MachinaScalar, Dstt: MachinaScalar}
  codira_name: logicalCast
  protection: internal
  shape_fn: ShapeLogicalCast
  lower_fn: LowerLogicalCast
  result_dtype: destType

- def: "logicalNot(_ input: Tensor<Bool>) -> Tensor<Bool>"
  x10_enum: at::aten::bitwise_not
  shape_fn: input
  lower_fn: xla::Not

- def: "logicalOr(_ lhs: Tensor<Bool>, _ rhs: Tensor<Bool>) -> Tensor<Bool>"
  x10_enum: at::aten::logical_or
  lower_fn: LowerBinaryOp<xla::Or>

- def: "lt(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<Bool>"
  generics: {T: MachinaNumeric}
  codira_name: less
  lower_fn: LowerBinaryOp<xla::Lt>
  result_dtype: Bool

- def: "matmul(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<T>"
  generics: {T: MachinaNumeric}
  lower_fn: LowerBinaryValueOp<CreateMatMul>

- def: "max(_ input: Tensor<T>, dim: Int64, keepDim: Bool) -> Tensor<T>"
  extras: ["canonicalize dim input"]
  generics: {T: MachinaNumeric}
  lower_fn: BuildMaxInDim

- def: "maximum(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<T>"
  x10_enum: at::aten::max
  generics: {T: MachinaNumeric}
  lower_fn: LowerBinaryOp<xla::Max>

- def: "mean(_ input: Tensor<T>, reductionIndices: [Int64], keepDims: Bool) -> Tensor<T>"
  extras: ["canonicalize reductionIndices input"]
  lower_fn: BuildMean
  generics: {T: MachinaNumeric}

- def: "min(_ input: Tensor<T>, dim: Int64, keepDim: Bool) -> Tensor<T>"
  extras: ["canonicalize dim input"]
  generics: {T: MachinaNumeric}
  lower_fn: BuildMinInDim

- def: "minimum(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<T>"
  x10_enum: at::aten::min
  generics: {T: MachinaNumeric}
  lower_fn: LowerBinaryOp<xla::Min>

- def: "mm(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<T>"
  codira_name: matMul
  generics: {T: MachinaNumeric}
  lower_fn: xla::Dot

- def: "mul(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<T>"
  generics: {T: MachinaNumeric}
  lower_fn: LowerBinaryOp<xla::Mul>

- def: "ne(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<Bool>"
  generics: {T: MachinaScalar}
  codira_name: notEqual
  lower_fn: LowerBinaryOp<xla::Ne>
  result_dtype: Bool

- def: "neg(_ input: Tensor<T>) -> Tensor<T>"
  generics: {T: MachinaNumeric}
  shape_fn: input
  lower_fn: xla::Neg

- def: "nll_loss(logits: Tensor, labels: Tensor, ignore_index: Int64) -> Tensor"
  lower_fn: LowerNllLoss

- def: "permute_value(_ input: Tensor<T>, dims: [Int64]) -> Tensor<T>"
  x10_enum: at::aten::permute
  codira_name: permute
  generics: {T: MachinaScalar}
  lower_fn: xla::Transpose

- def: "physical_cast(_ input: Tensor<T>, destType: ScalarType) -> Tensor<T>"
  generics: {T: MachinaScalar}
  codira_name: physicalCast
  protection: internal
  x10_enum: xla_symbols::cast
  shape_fn: ShapeLogicalCast
  lower_fn: LowerLogicalCast

- def: "pow(_ input: Tensor<T>, _ other: Tensor<T>) -> Tensor<T>"
  generics: {T: MachinaNumeric}
  lower_fn: xla::Pow

- def: "prod(_ input: Tensor<T>, reductionIndices: [Int64], keepDims: Bool) -> Tensor<T>"
  extras: ["canonicalize reductionIndices input"]
  generics: {T: MachinaNumeric}
  lower_fn: LowerProd

- def: "qr(_ input: Tensor<T>, fullMatrices: Bool) -> (q: Tensor<T>, r: Tensor<T>)"
  generics: {T: FloatingPoint & MachinaScalar}
  lower_fn: LowerQR

- def: "relu(features: Tensor<T>) -> Tensor<T>"
  generics: {T: MachinaNumeric}
  lower_fn: BuildRelu

- def: "rem(_ input: Tensor<T>, _ other: Tensor<T>) -> Tensor<T>"
  codira_name: mod
  x10_enum: at::aten::xla_rem
  generics: {T: MachinaNumeric}
  lower_fn: xla::Rem

- def: "repeat(_ input: Tensor<T>, multiples: [Int64]) -> Tensor<T>"
  codira_name: tile
  generics: {T: MachinaScalar}
  lower_fn: BuildRepeat

- def: "resize_value(_ input: Tensor<T>, dims: [Int64]) -> Tensor<T>"
  x10_enum: at::aten::resize
  generics: {T: MachinaScalar}
  lower_fn: BuildResize

- def: "round_to_even(_ input: Tensor<T>) -> Tensor<T>"
  codira_name: round
  generics: {T: MachinaNumeric}
  lower_fn: xla::RoundToEven
  shape_fn: input

- def: "rsqrt(_ input: Tensor<T>) -> Tensor<T>"
  generics: {T: FloatingPoint & MachinaScalar}
  shape_fn: input
  lower_fn: xla::Rsqrt

- def: "select(_ input: Tensor<T>, dim: Int64, index: Int64) -> Tensor<T>"
  generics: {T: MachinaScalar}
  extras: ["canonicalize dim input"]
  lower_fn: LowerSelect

- def: "sigmoid(_ input: Tensor<T>) -> Tensor<T>"
  generics: {T: FloatingPoint & MachinaScalar}
  shape_fn: input
  lower_fn: BuildSigmoid

- def: "sign(_ input: Tensor<T>) -> Tensor<T>"
  generics: {T: MachinaNumeric}
  shape_fn: input
  lower_fn: BuildSign

- def: "sin(_ input: Tensor<T>) -> Tensor<T>"
  generics: {T: FloatingPoint & MachinaScalar}
  shape_fn: input
  lower_fn: xla::Sin

- def: "sinh(_ input: Tensor<T>) -> Tensor<T>"
  generics: {T: FloatingPoint & MachinaScalar}
  shape_fn: input
  lower_fn: xla::Sinh

- def: "slice(_ input: Tensor<T>, dim: Int64, start: Int64, end: Int64, stride: Int64) -> Tensor<T>"
  extras: ["canonicalize dim input"]
  generics: {T: MachinaScalar}
  shape_fn: ShapeSlice
  lower_fn: LowerSlice

- def: "softmax(_ input: Tensor<T>, dim: Int64) -> Tensor<T>"
  extras: ["canonicalize dim input"]
  generics: {T: FloatingPoint & MachinaScalar}
  lower_fn: BuildSoftmax
  shape_fn: input

- def: "sqrt(_ input: Tensor<T>) -> Tensor<T>"
  generics: {T: FloatingPoint & MachinaScalar}
  shape_fn: input
  lower_fn: xla::Sqrt

- def: "squeeze(_ input: Tensor<T>, dim: Int64) -> Tensor<T>"
  extras: ["canonicalize dim input"]
  generics: {T: MachinaScalar}
  lower_fn: LowerSqueeze

- def: "stack(_ input: [Tensor<T>], dim: Int64) -> Tensor<T>"
  extras: ["canonicalize dim input CanonicalizeStack"]
  generics: {T: MachinaScalar}
  lower_fn: BuildStack

- def: "sub(_ lhs: Tensor<T>, _ rhs: Tensor<T>) -> Tensor<T>"
  generics: {T: MachinaNumeric}
  lower_fn: LowerBinaryOp<xla::Sub>

- def: "sum(_ input: Tensor<T>, reductionIndices: [Int64], keepDims: Bool) -> Tensor<T>"
  extras: ["canonicalize reductionIndices input"]
  generics: {T: MachinaNumeric}
  lower_fn: BuildSum

- def: "svd(_ input: Tensor<T>, computeUv: Bool, fullMatrices: Bool) -> (s: Tensor<T>, u: Tensor<T>, v: Tensor<T>)"
  generics: {T: FloatingPoint & MachinaScalar}
  lower_fn: LowerSVD

- def: "tan(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  generics: {T: MachinaNumeric}
  lower_fn: xla::Tan

- def: "tanh(_ input: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  generics: {T: FloatingPoint & MachinaScalar}
  lower_fn: xla::Tanh

- def: "tf_Conv(_ input: Tensor<T>, _ filter: Tensor<T>, _ depthwise: Bool, _ strides: [Int64], _ padding: TFPadding, _ explicit_paddings: [Int64], _ data_format: TFDataFormat, _ dilations: [Int64]) -> Tensor<T>"
  x10_enum: at::aten::tf_convolution
  generics: {T: MachinaNumeric}
  protection: internal
  lower_fn: BuildTfConv

- def: "tf_ConvBackpropFilter(_ input: Tensor<T>, _ filter_sizes: [Int64], _ out_backprop: Tensor<T>, _ depthwise: Bool, _ strides: [Int64], _ padding: TFPadding, _ explicit_paddings: [Int64], _ data_format: TFDataFormat, _ dilations: [Int64]) -> Tensor<T>"
  x10_enum: at::aten::tf_conv_backprop_filter
  generics: {T: MachinaNumeric}
  protection: internal
  lower_fn: BuildTfConvBackpropFilter

- def: "tf_ConvBackpropInput(_ input_sizes: [Int64], _ filter: Tensor<T>, _ out_backprop: Tensor<T>, _ depthwise: Bool, _ strides: [Int64], _ padding: TFPadding, _ explicit_paddings: [Int64], _ data_format: TFDataFormat, _ dilations: [Int64]) -> Tensor<T>"
  x10_enum: at::aten::tf_conv_backprop_input
  generics: {T: MachinaNumeric}
  protection: internal
  lower_fn: BuildTfConvBackpropInput

- def: "tf_MirrorPad(_ input: Tensor<T>, _ padding: [Int64], _ mode: TFMirrorPadMode) -> Tensor<T>"
  x10_enum: at::aten::tf_mirror_pad
  generics: {T: MachinaScalar}
  protection: internal
  lower_fn: BuildMirrorPad

- def: "tf_MirrorPadGrad(_ grad_output: Tensor<T>, _ input_size: [Int64], _ padding: [Int64], _ mode: TFMirrorPadMode) -> Tensor<T>"
  x10_enum: at::aten::tf_mirror_pad_backward
  generics: {T: MachinaScalar}
  protection: internal
  lower_fn: BuildMirrorPadBackward

- def: "tf_OneHot(_ indices: Tensor<Ti>, _ onValue: Tensor<T>, _ offValue: Tensor<T>, _ depth: Int64, _ axis: Int64) -> Tensor<T>"
  result_dtype: onValue
  generics: {Ti: MachinaInteger, T: MachinaScalar}
  protection: internal
  x10_enum: at::aten::tf_one_hot
  lower_fn: BuildOneHot

- def: "tf_StatelessRandomNormal(_ shape: [Int64], _ seeds: Tensor<Ti>, dtype: ScalarType) -> Tensor<T>"
  x10_enum: at::aten::tf_stateless_random_normal
  generics: {T: MachinaScalar, Ti: MachinaIndex}
  extras: ["shape_fn shape", "needs_lowering_context"]
  lower_fn: LowerTfStatelessRandomNormal
  protection: internal
  result_dtype: dtype

- def: "tf_StatelessRandomUniform(_ shape: [Int64], _ seeds: Tensor<Ti>, _ minvalue: Tensor<T>, _ maxvalue: Tensor<T>) -> Tensor<T>"
  x10_enum: at::aten::tf_stateless_random_uniform
  generics: {T: MachinaScalar, Ti: MachinaIndex}
  extras: ["shape_fn shape", "needs_lowering_context"]
  lower_fn: LowerTfStatelessRandomUniform
  protection: internal
  result_dtype: minvalue

- def: "tf_UnsortedSegmentSum(_ data: Tensor<T>, indicies: Tensor<Ti>, numSegments: Int64) -> Tensor<T>"
  generics: {T: MachinaNumeric, Ti: MachinaIndex}
  x10_enum: at::aten::tf_unsorted_segment_sum
  protection: internal
  lower_fn: LowerTfUnsortedSegmentSum

- def: "threshold(_ input: Tensor<T>, output: Tensor<T>, threshold: Float, value: Float) -> Tensor<T>"
  x10_enum: at::aten::threshold_backward
  generics: {T: MachinaNumeric}
  protection: internal
  shape_fn: input
  lower_fn: BuildThreshold

- def: "topk(input: Tensor<T>, k: Int64, dim: Int64, largest: Bool) -> (Tensor<T>, Tensor<Int64>)"
  extras: ["canonicalize dim input"]
  lower_fn: BuildTopK
  generics: {T: FloatingPoint & MachinaScalar}
  result_dtype: [input, Long]

- def: "truncated_normal(_ input: Tensor<T>) -> Tensor<T>"
  x10_enum: at::aten::xla_truncated_normal
  generics: {T: FloatingPoint & MachinaScalar}
  protection: internal
  codira_name: truncatedNormal
  shape_fn: input
  lower_fn: machina::TruncatedNormal

- def: "update_slice(input: Tensor<T>, source: Tensor<T>, baseIndices: [Int64]) -> Tensor<T>"
  x10_enum: xla_symbols::update_slice
  codira_name: updateSlice
  generics: {T: MachinaScalar}
  lower_fn: BuildUpdateSlice

- def: "where(condition: Tensor<Bool>, input: Tensor<T>, other: Tensor<T>) -> Tensor<T>"
  shape_fn: input
  generics: {T: MachinaScalar}
  codira_name: where_
  lower_fn: LowerWhere

- def: "xla_pad(_ input: Tensor<T>, paddingValue: AnyScalar, paddingConfig: [PaddingConfigDimension]) -> Tensor<T>"
  generics: {T: MachinaScalar}
  protection: internal
  codira_name: xlaPad
  lower_fn: LowerPad

- def: "xla_slice(_ input: Tensor<T>, start_indices: [Int64], limit_indices: [Int64], strides: [Int64]) -> Tensor<T>"
  codira_name: xlaSlice
  generics: {T: MachinaScalar}
  lower_fn: xla::Slice
