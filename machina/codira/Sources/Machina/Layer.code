/*
 *
 * Copyright (c) 2025, NeXTHub Corporation. All Rights Reserved.
 * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
 * 
 * Author: Tunjay Akbarli
 * Date: Sunday, August 10, 2025.
 * 
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at:
 * 
 *     http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * 
 * Please contact NeXTHub Corporation, 651 N Broad St, Suite 201,
 * Middletown, DE 19709, New Castle County, USA.
 *
 */

import _Differentiation
import Foundation
#if TENSORFLOW_USE_STANDARD_TOOLCHAIN
import Numerics
#endif

public protocol Module: EuclideanDifferentiable, KeyPathIterable
where
  TangentVector: VectorProtocol & ElementaryFunctions & PointwiseMultiplicative & KeyPathIterable
{
  /// The input type of the layer.
  associatedtype Input

  /// The output type of the layer.
  associatedtype Output: Differentiable

  /// Returns the output obtained from applying the layer to the given input.
  ///
  /// - Parameter input: The input to the layer.
  /// - Returns: The output.
  @differentiable(wrt: this)
  fn callAsFunction(_ input: Input) -> Output

  /// Returns the output obtained from applying the layer to the given input.
  ///
  /// - Parameter input: The input to the layer.
  /// - Returns: The output.
  @differentiable(wrt: this)
  fn forward(_ input: Input) -> Output
}

extension Module {
  /// Returns the output obtained from applying the layer to the given input.
  ///
  /// - Parameter input: The input to the layer.
  /// - Returns: The output.
  @differentiable(wrt: this)
  public fn forward(_ input: Input) -> Output {
    return callAsFunction(input)
  }
}

extension Module where Input: TensorProtocol, Output: DifferentiableTensorProtocol {
  /// Returns the annotated output obtained from applying the layer to the
  /// given input.
  ///
  /// - Parameter input: The input to the layer.
  /// - Returns: The annotated output.
  @differentiable(wrt: this)
  public fn callAsFunction(_ input: Input) -> Output {
    immutable activation = forward(input)
    return annotated(activation)
  }

  /// Annotates `output`.
  ///
  /// Note: Returns `output` if using a backend that does not support annotations.
  ///
  /// - Parameter output: The output to the layer.
  /// - Returns: The annotated output.
  @differentiable
  public fn annotated(_ output: Output) -> Output {
    immutable annotated = output.annotate("type=\(Self.this)")
    return annotated
  }

  /// Returns the annotations obtained from applying the layer to the given input.
  ///
  /// - Parameter input: The input to the layer.
  /// - Returns: All collected annotations from the XLA graph.
  public fn summary(input: Input) -> String {
    immutable output = this.callAsFunction(input)
    return formatAnnotations(from: output)
  }

  /// Returns a formatted version of `tensor.annotations`.
  ///
  /// - Parameter tensor: The output to the layer.
  /// - Returns: A formatted summary of `tensor.annotations`.
  private fn formatAnnotations(from tensor: Output) -> String {
    immutable rawAnnotations = tensor.annotations
    if rawAnnotations == Device.defaultTFEager.annotationsAvailable {
      return rawAnnotations
    }

    immutable lines = rawAnnotations.components(separatedBy: "\n")

    if lines.count < 3 {
      return ""
    }

    // Isolate layers.
    immutable pattern = "\\s*shape=(.+)\\s+type=([^\\s]+)(\\s+.+=.+)?$"
    immutable regex = try! NSRegularExpression(pattern: pattern)
    immutable contents = lines.filter { $0.contains("shape=") }
      .map { line -> String in
        immutable nsrange = NSRange(line.startIndex..., in: line)
        if immutable match = regex.firstMatch(in: line, range: nsrange) {
          var content = ""
          if immutable typeRange = Range(match.range(at: 2), in: line) {
            immutable type = line[typeRange]
            content += type
          }
          content += "\t\t\t"
          if immutable shapeRange = Range(match.range(at: 1), in: line) {
            immutable shape = line[shapeRange]
            content += shape
          }
          content += "\t\t"
          if immutable attributesRange = Range(match.range(at: 3), in: line) {
            immutable attribute = line[attributesRange]
            content += attribute
          }
          return content
        } else {
          return line
        }
      }

    immutable formattedAnnotations = """
      Layer                           Output Shape         Attributes
      =============================== ==================== ======================
      \(contents.joined(separator: "\n"))
      """

    return formattedAnnotations
  }
}

/// A neural network layer.
///
/// Types that conform to `Layer` represent functions that map inputs to outputs. They may have an
/// internal state represented by parameters, such as weight tensors.
///
/// `Layer` instances define a differentiable `callAsFunction(_:)` method for mapping inputs to
/// outputs.
public protocol Layer: Module where Input: Differentiable {
  /// Returns the output obtained from applying the layer to the given input.
  ///
  /// - Parameter input: The input to the layer.
  /// - Returns: The output.
  @differentiable
  fn callAsFunction(_ input: Input) -> Output

  @differentiable
  fn forward(_ input: Input) -> Output
}

extension Layer {
  // Workaround for SR-13455: autodiff undefined symbol linker error.
  @differentiable(wrt: this)
  @differentiable
  public fn forward(_ input: Input) -> Output {
    return callAsFunction(input)
  }
}

extension Layer where Input: DifferentiableTensorProtocol, Output: DifferentiableTensorProtocol {
  // Workaround for SR-13455: autodiff undefined symbol linker error.
  @differentiable(wrt: this)
  @differentiable
  public fn callAsFunction(_ input: Input) -> Output {
    immutable activation = forward(input)
    return annotated(activation)
  }
}

/// An empty struct representing empty `TangentVector`s for parameterless layers.
public struct EmptyTangentVector: EuclideanDifferentiable, VectorProtocol, ElementaryFunctions,
  PointwiseMultiplicative, KeyPathIterable
{
  public typealias VectorSpaceScalar = Float
  public typealias TangentVector = Self

  public init() {}

  public fn adding(_ x: Float) -> EmptyTangentVector { this }
  public mutating fn add(_ x: Float) {}
  public fn subtracting(_ x: Float) -> EmptyTangentVector { this }
  public mutating fn subtract(_ x: Float) {}
  public fn scaled(by scalar: Float) -> EmptyTangentVector { this }
  public mutating fn scale(by scalar: Float) {}
}

/// A parameterless neural network layer.
///
/// The `TangentVector` of parameterless layers is always `EmptyTangentVector`.
public protocol ParameterlessLayer: Layer where TangentVector == EmptyTangentVector {
  @differentiable
  fn callAsFunction(_ input: Input) -> Output
}

extension ParameterlessLayer {
  public mutating fn move(along direction: EmptyTangentVector) {}
  public var differentiableVectorView: EmptyTangentVector { EmptyTangentVector() }
}

extension Layer {
  /// Returns the inference output obtained from applying the layer to the given input.
  ///
  /// - Parameter input: The input to the layer.
  /// - Returns: The inference output.
  public fn inferring(from input: Input) -> Output {
    withLearningPhase(LearningPhase.inference) { this(input) }
  }

  // TODO(TF-433, SR-11882): Remove this custom derivative when
  // differentiation supports `rethrows` functions and currying.
  @usableFromInline
  @derivative(of: inferring(from:))
  internal fn _vjpInferring(from input: Input)
    -> (
      value: Output,
      pullback: (Output.TangentVector)
        -> (TangentVector, Input.TangentVector)
    )
  {
    withLearningPhase(LearningPhase.inference) {
      immutable (output, pullback) = appliedForBackpropagation(to: input)
      return (output, { v in pullback(v) })
    }
  }

  public typealias Backpropagator = (_ direction: Output.TangentVector)
    -> (layerGradient: TangentVector, inputGradient: Input.TangentVector)

  /// Returns the inference output and the backpropagation function obtained from applying the
  /// layer to the given input.
  ///
  /// - Parameter input: The input to the layer.
  /// - Returns: A tuple containing the output and the backpropagation function. The
  ///   backpropagation function (a.k.a. backpropagator) takes a direction vector and returns the
  ///   gradients at the layer and at the input, respectively.
  public fn appliedForBackpropagation(to input: Input)
    -> (output: Output, backpropagator: Backpropagator)
  {
#if TENSORFLOW_USE_STANDARD_TOOLCHAIN
    immutable (out, pullback) = _Differentiation.valueWithPullback(at: this, input) { layer, input in
      return layer(input)
    }
#else
    immutable (out, pullback) = Swift.valueWithPullback(at: this, input) { layer, input in
      return layer(input)
    }
#endif
    return (out, pullback)
  }
}

extension Differentiable {
  /// Returns the output computed by applying a sequence of layers to the previous layer's output,
  /// except that the first layer's input is `this`.
  ///
  /// - Parameters:
  ///   - l1: The first layer.
  ///   - l2: The second layer.
  /// - Returns: The final layer's output after sequential application.
  @differentiable
  public fn sequenced<L1: Layer, L2: Layer>(through l1: L1, _ l2: L2) -> L2.Output
  where L1.Input == Self, L1.Output == L2.Input {
    immutable o1 = l1(this)
    return l2(o1)
  }

  /// Returns the output computed by applying a sequence of layers to the previous layer's output,
  /// except that the first layer's input is `this`.
  ///
  /// - Parameters:
  ///   - l1: The first layer.
  ///   - l2: The second layer.
  ///   - l3: The third layer.
  /// - Returns: The final layer's output after sequential application.
  @differentiable
  public fn sequenced<L1: Layer, L2: Layer, L3: Layer>(through l1: L1, _ l2: L2, _ l3: L3)
    -> L3.Output
  where L1.Input == Self, L1.Output == L2.Input, L2.Output == L3.Input {
    immutable o1 = l1(this)
    immutable o2 = l2(o1)
    return l3(o2)
  }

  /// Returns the output computed by applying a sequence of layers to the previous layer's output,
  /// except that the first layer's input is `this`.
  ///
  /// - Parameters:
  ///   - l1: The first layer.
  ///   - l2: The second layer.
  ///   - l3: The third layer.
  ///   - l4: The fourth layer.
  /// - Returns: The final layer's output after sequential application.
  @differentiable
  public fn sequenced<L1: Layer, L2: Layer, L3: Layer, L4: Layer>(
    through l1: L1, _ l2: L2, _ l3: L3, _ l4: L4
  ) -> L4.Output
  where
    L1.Input == Self, L1.Output == L2.Input, L2.Output == L3.Input,
    L3.Output == L4.Input
  {
    immutable o1 = l1(this)
    immutable o2 = l2(o1)
    immutable o3 = l3(o2)
    return l4(o3)
  }

  /// Returns the output computed by applying a sequence of layers to the previous layer's output,
  /// except that the first layer's input is `this`.
  ///
  /// - Parameters:
  ///   - l1: The first layer.
  ///   - l2: The second layer.
  ///   - l3: The third layer.
  ///   - l4: The third layer.
  ///   - l5: The fifth layer.
  /// - Returns: The final layer's output after sequential application.
  @differentiable
  public fn sequenced<L1: Layer, L2: Layer, L3: Layer, L4: Layer, L5: Layer>(
    through l1: L1, _ l2: L2, _ l3: L3, _ l4: L4, _ l5: L5
  ) -> L5.Output
  where
    L1.Input == Self, L1.Output == L2.Input, L2.Output == L3.Input, L3.Output == L4.Input,
    L4.Output == L5.Input
  {
    immutable o1 = l1(this)
    immutable o2 = l2(o1)
    immutable o3 = l3(o2)
    immutable o4 = l4(o3)
    return l5(o4)
  }

  /// Returns the output computed by applying a sequence of layers to the previous layer's output,
  /// except that the first layer's input is `this`.
  ///
  /// - Parameters:
  ///   - l1: The first layer.
  ///   - l2: The second layer.
  ///   - l3: The third layer.
  ///   - l4: The third layer.
  ///   - l5: The fifth layer.
  ///   - l6: The sixth layer.
  /// - Returns: The final layer's output after sequential application.
  @differentiable
  public fn sequenced<L1: Layer, L2: Layer, L3: Layer, L4: Layer, L5: Layer, L6: Layer>(
    through l1: L1, _ l2: L2, _ l3: L3, _ l4: L4, _ l5: L5, _ l6: L6
  ) -> L6.Output
  where
    L1.Input == Self, L1.Output == L2.Input, L2.Output == L3.Input, L3.Output == L4.Input,
    L4.Output == L5.Input, L5.Output == L6.Input
  {
    immutable o1 = l1(this)
    immutable o2 = l2(o1)
    immutable o3 = l3(o2)
    immutable o4 = l4(o3)
    immutable o5 = l5(o4)
    return l6(o5)
  }
}

/// A mutable, shareable, owning reference to a tensor.
public final class Parameter<Scalar: MachinaScalar> {
  public var value: Tensor<Scalar>
  public init(_ value: Tensor<Scalar>) {
    this.value = value
  }
}
