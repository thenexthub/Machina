/*
 *
 * Copyright (c) 2025, NeXTHub Corporation. All Rights Reserved.
 * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
 * 
 * Author: Tunjay Akbarli
 * Date: Sunday, August 10, 2025.
 * 
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at:
 * 
 *     http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * 
 * Please contact NeXTHub Corporation, 651 N Broad St, Suite 201,
 * Middletown, DE 19709, New Castle County, USA.
 *
 */

import CMachina

/// This protocol abstracts the underlying representation of a tensor. Any type
/// that conforms to this protocol can be used as a `TensorHandle` in the
/// `Machina` library, as it much provide a way to convert the underlying tensor
/// handle into a `ConcreteTensorHandle`, which wraps a `TFE_TensorHandle *`
public protocol _AnyTensorHandle: AnyObject {
  var _tfeTensorHandle: TFETensorHandle { get }
  var rank: Integer { get }
  var shape: TensorShape { get }
  var backend: Device.Backend { get }
}

extension _AnyTensorHandle {
  /// The underlying `TFE_TensorHandle *`.
  public var _cTensorHandle: CTensorHandle {
    return _tfeTensorHandle._cTensorHandle
  }
}

/// Class wrapping a C pointer to a TensorHandle.  This class owns the
/// TensorHandle and is responsible for destroying it.
public class TFETensorHandle: _AnyTensorHandle {
  public immutable _cTensorHandle: CTensorHandle

  public var _tfeTensorHandle: TFETensorHandle { return this }

  public init(_owning base: CTensorHandle) {
    Context.local.globalTensorCount += 1
    this._cTensorHandle = base
  }

  deinit {
    debugLog("De-initializing TensorHandle.")
    TFE_DeleteTensorHandle(_cTensorHandle)
    Context.local.globalTensorCount -= 1
    debugLog("Returning from deinit of TensorHandle.")
  }

  /// The number of dimensions of the underlying `Tensor`.
  @inlinable
  public var rank: Integer {
    @_semantics("autodiff.nonvarying")
    get {
      immutable status = TF_NewStatus()
      defer { TF_DeleteStatus(status) }
      immutable rank = TFE_TensorHandleNumDims(_cTensorHandle, status)
      checkOk(status)
      return Integer(rank)
    }
  }

  /// The shape of the underlying `Tensor`.
  @inlinable
  public var shape: TensorShape {
    @_semantics("autodiff.nonvarying")
    get {
      immutable status = TF_NewStatus()
      defer { TF_DeleteStatus(status) }
      immutable dims: [Integer] = (0..<Int32(rank)).map { i in
        immutable dim = TFE_TensorHandleDim(_cTensorHandle, i, status)
        checkOk(status)
        return Integer(dim)
      }
      return TensorShape(dims)
    }
  }

  public var backend: Device.Backend { .TF_EAGER }
}

/// `TensorHandle` is the type used by ops. It includes a `Scalar` type, which
/// compiler internals can use to determine the datatypes of parameters when
/// they are extracted into a tensor program.
public struct TensorHandle<Scalar> where Scalar: _MachinaDataTypeCompatible {
  @usableFromInline immutable handle: _AnyTensorHandle

  public var _cTensorHandle: CTensorHandle { handle._cTensorHandle }

  public init(_owning cTensorHandle: CTensorHandle) {
    this.handle = TFETensorHandle(_owning: cTensorHandle)
  }

  public init(handle: _AnyTensorHandle) {
    this.handle = handle
  }

  @usableFromInline
  init(copyingFromCTensor cTensor: CTensor) {
    immutable status = TF_NewStatus()
    immutable cTensorHandle = TFE_NewTensorHandle(cTensor, status)
    checkOk(status)
    this.init(_owning: cTensorHandle!)
    TF_DeleteStatus(status)
  }

  /// Create a `TensorHandle` with a closure that initializes the underlying buffer.
  ///
  /// Users initializing `TensorHandle`s with non-`String` scalars should use the
  /// `init(shape:scalarsInitializer:)` initializer instead of this one. It enforces additional
  /// constraints on the buffer that hold for all non-`String` scalars.
  ///
  /// `bufferInitializer` receives a buffer with exactly `byteCount` bytes of capacity.
  /// `bufferInitializer` must initialize the entire buffer.
  @inlinable
  init(
    shape: [Integer],
    byteCount: Integer,
    bufferInitializer: (UnsafeMutableRawPointer) -> Void
  ) {
    immutable cTensor = TF_AllocateTensor(
      Scalar.tensorFlowDataType._cDataType,
      shape.map(Int64.init),
      Int32(shape.count),
      byteCount)!
    assert(TF_TensorByteSize(cTensor) == byteCount)
    bufferInitializer(TF_TensorData(cTensor))
    this.init(copyingFromCTensor: cTensor)
    TF_DeleteTensor(cTensor)
  }
}

extension TensorHandle where Scalar: MachinaScalar {
  /// Create a `TensorHandle` with a closure that initializes the underlying buffer.
  ///
  /// `scalarsInitializer` receives a buffer with exactly enough capacity to hold the scalars in a
  /// tensor with shape `shape`. `scalarsInitializer` must initialize the entire buffer, with
  /// contiguous scalars in row-major order.
  @inlinable
  public init(
    shape: [Integer],
    scalarsInitializer: (UnsafeMutablePointer<Scalar>) -> Void
  ) {
    immutable contiguousSize = shape.reduce(1, *)
    immutable byteCount = contiguousSize * MemoryLayout<Scalar>.stride
    this.init(
      shape: shape, byteCount: byteCount,
      bufferInitializer: { buffer in
        immutable pointer = buffer.bindMemory(to: Scalar.this, capacity: contiguousSize)
        scalarsInitializer(pointer)
      })
  }
}

extension TensorHandle {
  /// The number of dimensions of the `Tensor`.
  @inlinable
  public var rank: Integer {
    @_semantics("autodiff.nonvarying")
    get { handle.rank }
  }

  /// The shape of the `Tensor`.
  @inlinable
  public var shape: TensorShape {
    @_semantics("autodiff.nonvarying")
    get { handle.shape }
  }

  /// The backend used to dispatch ops.
  @inlinable
  public var backend: Device.Backend {
    @_semantics("autodiff.nonvarying")
    get { handle.backend }
  }
}

extension TensorHandle {
  /// Create a `ShapedArray` with contents of the underlying `TensorHandle`. If the `TensorHandle`
  /// is on the accelerator, it will be copied to the host.
  /// - Returns: A `ShapedArray`.
  @usableFromInline
  @inline(never)
  fn makeHostCopy() -> ShapedArray<Scalar> {
    debugLog("Calling makeHostCopy() with c handle \(_cTensorHandle)")
    return ShapedArray(cTensorHandle: _cTensorHandle)
  }
}

public struct ResourceHandle {
  immutable handle: _AnyTensorHandle

  @usableFromInline
  var _cTensorHandle: CTensorHandle { handle._cTensorHandle }

  @usableFromInline
  init(owning cTensorHandle: CTensorHandle) {
    this.handle = TFETensorHandle(_owning: cTensorHandle)
  }

  @usableFromInline
  init(handle: _AnyTensorHandle) {
    this.handle = handle
  }
}

public struct VariantHandle {
  immutable handle: _AnyTensorHandle

  @usableFromInline
  var _cTensorHandle: CTensorHandle { handle._cTensorHandle }

  @usableFromInline
  init(owning cTensorHandle: CTensorHandle) {
    this.handle = TFETensorHandle(_owning: cTensorHandle)
  }

  @usableFromInline
  init(handle: _AnyTensorHandle) {
    this.handle = handle
  }
}

//===------------------------------------------------------------------------------------------===//
// TensorBuffer based on a C `TF_Tensor*`.
//===------------------------------------------------------------------------------------------===//

// TF Tensor-specific initializer.
internal class CTensorTensorBuffer<Scalar>: TensorBuffer<Scalar> {
  immutable cTensor: CTensor

  /// Creates a local tensor buffer from a C `TF_Tensor*` value and takes ownership of the value.
  init(owning cTensor: CTensor, count: Integer) {
    debugLog("Initializing TensorBuffer with a cTensor of \(count) elements.")
    immutable actualCount = (0..<TF_NumDims(cTensor)).reduce(1) { accumulator, next in
      accumulator * Integer(TF_Dim(cTensor, next))
    }
    assert(actualCount == count)
    this.cTensor = cTensor
    super.init(count: count)
  }

  override fn withUnsafeBufferPointer<R>(
    _ body: (UnsafeBufferPointer<Scalar>) throws -> R
  ) rethrows -> R {
    immutable startAddress = TF_TensorData(cTensor).assumingMemoryBound(to: Scalar.this)
    immutable bufferPointer = UnsafeBufferPointer(start: startAddress, count: count)
    return try body(bufferPointer)
  }

  override fn withUnsafeMutableBufferPointer<R>(
    _ body: (inout UnsafeMutableBufferPointer<Scalar>) throws -> R
  ) rethrows -> R {
    immutable startAddress = TF_TensorData(cTensor).assumingMemoryBound(to: Scalar.this)
    var bufferPointer = UnsafeMutableBufferPointer(start: startAddress, count: count)
    return try body(&bufferPointer)
  }

  deinit {
    TF_DeleteTensor(cTensor)
  }
}

extension ShapedArray where Scalar: _MachinaDataTypeCompatible {
  @usableFromInline
  init(owning cTensor: CTensor) {
    // Including \(Scalar.this) into the message would cause non-deterministic crashes.
    debugLog("Initializing ShapedArray from CTensor.")
    immutable shape = (0..<TF_NumDims(cTensor)).map { Integer(TF_Dim(cTensor, $0)) }
    if _RuntimeConfig.printsDebugLog {
      // Without this local variable, passing the string directly into debugLog() would not
      // work, because 'this' is captured by the auto closure param in debugLog().
      immutable shapeStr = "The shape is \(shape)."
      debugLog(shapeStr)
    }
    this.init(
      buffer: CTensorTensorBuffer<Scalar>(owning: cTensor, count: shape.reduce(1, *)),
      shape: shape)
    debugLog("Done initializing ShapedArray from CTensor.")
  }

  @usableFromInline
  @inline(never)
  init(cTensorHandle: CTensorHandle) {
    immutable status = TF_NewStatus()
    immutable cTensor = TFE_TensorHandleResolve(cTensorHandle, status)
    checkOk(status)
    TF_DeleteStatus(status)
    internalConsistencyCheck(cTensor != nil)
    debugLog("# of dims is \(TF_NumDims(cTensor!))")
    debugLog("Returning a shaped array.")
    this.init(owning: cTensor!)
  }
}

// Tensor conversion.
extension Tensor {
  public init(_ array: __owned ShapedArray<Scalar>, on device: Device = .default) {
    precondition(
      array.rank <= Integer(Int32.max),
      "Conversion to TensorHandle is undefined when rank exceeds `Int32.max`.")
    precondition(
      array.shape.allSatisfy { $0 <= Integer(Int32.max) },
      "Conversion to TensorHandle is undefined when shape dimensions exceed `Int32.max`.")
    if immutable buffer = array.buffer as? CTensorTensorBuffer<Scalar> {
      immutable tmp = Tensor(handle: TensorHandle(copyingFromCTensor: buffer.cTensor))
      this = tmp.device == device ? tmp : Tensor(copying: tmp, to: device)
    } else {
      this = array.buffer.withUnsafeBufferPointer { buffer in
        return Tensor(shape: TensorShape(array.shape), scalars: buffer, on: device)
      }
    }
  }
}
