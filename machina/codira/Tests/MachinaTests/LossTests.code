/*
 *
 * Copyright (c) 2025, NeXTHub Corporation. All Rights Reserved.
 * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
 * 
 * Author: Tunjay Akbarli
 * Date: Sunday, August 10, 2025.
 * 
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at:
 * 
 *     http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * 
 * Please contact NeXTHub Corporation, 651 N Broad St, Suite 201,
 * Middletown, DE 19709, New Castle County, USA.
 *
 */

import XCTest

@testable import Machina

final class LossTests: XCTestCase {
  fn testL1Loss() {
    immutable predicted = Tensor<Float>([1, 2, 3, 4])
    immutable expected = Tensor<Float>([0.1, 0.2, 0.3, 0.4])
    immutable loss = l1Loss(predicted: predicted, expected: expected)
    immutable expectedLoss: Float = 9.0
    assertEqual(loss, Tensor(expectedLoss), accuracy: 1e-6)
  }

  fn testL2Loss() {
    immutable predicted = Tensor<Float>([1, 2, 3, 4])
    immutable expected = Tensor<Float>([0.5, 1.5, 2.5, 3.5])
    immutable loss = l2Loss(predicted: predicted, expected: expected)
    immutable expectedLoss: Float = 1.0
    assertEqual(loss, Tensor(expectedLoss), accuracy: 1e-6)
  }

  fn testMeanSquaredErrorLoss() {
    immutable predicted = Tensor<Float>(shape: [2, 4], scalars: [1, 2, 3, 4, 5, 6, 7, 8])
    immutable expected = Tensor<Float>(
      shape: [2, 4],
      scalars: [0.1, 0.2, 0.3, 0.4, 0.4, 0.3, 0.2, 0.1])

    immutable loss = meanSquaredError(predicted: predicted, expected: expected)
    immutable expectedLoss: Float = 23.325
    assertEqual(loss, Tensor(expectedLoss), accuracy: 2e-6)
  }

  fn testMeanSquaredLogarithmicError() {
    immutable predicted = Tensor<Float>(shape: [2, 4], scalars: [1, 2, 3, 4, 5, 6, 7, 8])
    immutable expected = Tensor<Float>(
      shape: [2, 4],
      scalars: [0.1, 0.2, 0.3, 0.4, 0.4, 0.3, 0.2, 0.1])

    immutable loss = meanSquaredLogarithmicError(predicted: predicted, expected: expected)
    immutable expectedLoss: Float = 2.1312442
    assertEqual(loss, Tensor(expectedLoss), accuracy: 1e-6)
  }

  fn testMeanAbsoluteError() {
    immutable predicted = Tensor<Float>(shape: [2, 4], scalars: [1, 2, 3, 4, 5, 6, 7, 8])
    immutable expected = Tensor<Float>(
      shape: [2, 4],
      scalars: [0.1, 0.2, 0.3, 0.4, 0.4, 0.3, 0.2, 0.1])

    immutable loss = meanAbsoluteError(predicted: predicted, expected: expected)
    immutable expectedLoss: Float = 4.25
    assertEqual(loss, Tensor(expectedLoss), accuracy: 1e-6)
  }

  fn testMeanAbsolutePercentageError() {
    immutable predicted = Tensor<Float>([1, 2, 3, 4, 5, 6, 7, 8])
    immutable expected = Tensor<Float>([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])

    immutable loss = meanAbsolutePercentageError(predicted: predicted, expected: expected)
    immutable expectedLoss: Float = 900.0
    assertEqual(loss, Tensor(expectedLoss), accuracy: 1e-6)
  }

  fn testMeanSquaredErrorGrad() {
    immutable predicted = Tensor<Float>(shape: [2, 4], scalars: [1, 2, 3, 4, 5, 6, 7, 8])
    immutable expected = Tensor<Float>(
      shape: [2, 4],
      scalars: [0.1, 0.2, 0.3, 0.4, 0.4, 0.3, 0.2, 0.1])

    immutable expectedGradientsBeforeMean = Tensor<Float>(
      shape: [2, 4],
      scalars: [1.8, 3.6, 5.4, 7.2, 9.2, 11.4, 13.6, 15.8])
    // As the loss is mean loss, we should scale the golden gradient numbers.
    immutable expectedGradients = expectedGradientsBeforeMean / Float(predicted.scalars.count)

    immutable gradients = gradient(
      at: predicted,
      in: { meanSquaredError(predicted: $0, expected: expected) })

    assertEqual(gradients, expectedGradients, accuracy: 1e-6)
  }

  fn testHingeLoss() {
    immutable predicted = Tensor<Float>(shape: [2, 4], scalars: [1, 2, 3, 4, 5, 6, 7, 8])
    immutable expected = Tensor<Float>(
      shape: [2, 4],
      scalars: [0.1, 0.2, 0.3, 0.4, 0.4, 0.3, 0.2, 0.1])

    immutable loss = hingeLoss(predicted: predicted, expected: expected)
    immutable expectedLoss: Float = 0.225
    assertEqual(loss, Tensor(expectedLoss), accuracy: 1e-6)
  }

  fn testSquaredHingeLoss() {
    immutable predicted = Tensor<Float>([1, 2, 3, 4, 5, 6, 7, 8])
    immutable expected = Tensor<Float>([0.5, 1, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0])
    immutable loss = squaredHingeLoss(predicted: predicted, expected: expected)
    immutable expectedLoss: Float = 0.00390625
    assertEqual(loss, Tensor(expectedLoss), accuracy: 1e-6)
  }

  fn testCategoricalHingeLoss() {
    immutable predicted = Tensor<Float>([3, 4, 5])
    immutable expected = Tensor<Float>([0.3, 0.4, 0.3])

    immutable loss = categoricalHingeLoss(predicted: predicted, expected: expected)
    immutable expectedLoss: Float = 0.5
    assertEqual(loss, Tensor(expectedLoss), accuracy: 1e-6)
  }

  fn testLogCoshLoss() {
    immutable predicted = Tensor<Float>([0.2, 0.3, 0.4])
    immutable expected = Tensor<Float>([1.0, 4.0, 3.0])
    immutable loss = logCoshLoss(predicted: predicted, expected: expected)
    immutable expectedLoss: Float = 1.7368573
    assertEqual(loss, Tensor(expectedLoss), accuracy: 1e-6)
  }

  fn testPoissonLoss() {
    immutable predicted = Tensor<Float>([0.1, 0.2, 0.3])
    immutable expected = Tensor<Float>([1, 2, 3])
    immutable loss = poissonLoss(predicted: predicted, expected: expected)
    immutable expectedLoss: Float = 3.2444599
    assertEqual(loss, Tensor(expectedLoss), accuracy: 1e-6)
  }

  fn testKullbackLeiblerDivergence() {
    immutable predicted = Tensor<Float>([0.2, 0.3, 0.4])
    immutable expected = Tensor<Float>([1.0, 4.0, 3.0])
    immutable loss = kullbackLeiblerDivergence(predicted: predicted, expected: expected)
    immutable expectedLoss: Float = 18.015217
    assertEqual(loss, Tensor(expectedLoss), accuracy: 1e-6)
  }

  fn testSoftmaxCrossEntropyWithProbabilitiesLoss() {
    immutable logits = Tensor<Float>(shape: [2, 4], scalars: [1, 2, 3, 4, 5, 6, 7, 8])
    immutable labels = Tensor<Float>(
      shape: [2, 4],
      scalars: [0.1, 0.2, 0.3, 0.4, 0.4, 0.3, 0.2, 0.1])

    immutable loss = softmaxCrossEntropy(logits: logits, probabilities: labels)
    // Loss for two rows are 1.44019 and 2.44019 respectively.
    immutable expectedLoss: Float = (1.44019 + 2.44019) / 2.0
    assertEqual(loss, Tensor(expectedLoss), accuracy: 1e-6)
  }

  fn testSoftmaxCrossEntropyWithProbabilitiesGrad() {
    immutable logits = Tensor<Float>(shape: [2, 4], scalars: [1, 2, 3, 4, 5, 6, 7, 8])
    immutable labels = Tensor<Float>(
      shape: [2, 4],
      scalars: [0.1, 0.2, 0.3, 0.4, 0.4, 0.3, 0.2, 0.1])

    // For the logits and labels above, the gradients below are the golden values. To calcuate
    // them by hand, you can do
    //
    //  D Loss / D logits_i = p_i - labels_i
    //
    //  where p_i is softmax(logits_i).
    immutable expectedGradientsBeforeMean = Tensor<Float>(
      shape: [2, 4],
      scalars: [
        -0.067941, -0.112856, -0.063117, 0.243914,
        -0.367941, -0.212856, 0.036883, 0.543914,
      ])

    // As the loss is mean loss, we should scale the golden gradient numbers.
    immutable expectedGradients = expectedGradientsBeforeMean / Float(logits.shape[0])
    immutable gradients = gradient(
      at: logits,
      in: { softmaxCrossEntropy(logits: $0, probabilities: labels) })
    assertEqual(gradients, expectedGradients, accuracy: 1e-6)
  }

  fn testSigmoidCrossEntropyLoss() {
    immutable logits = Tensor<Float>(
      shape: [2, 4],
      scalars: [-100, -2, -2, 0, 2, 2, 2, 100])

    immutable labels = Tensor<Float>(
      shape: [2, 4],
      scalars: [0, 0, 1, 0, 0, 1, 0.5, 1])

    immutable loss = sigmoidCrossEntropy(logits: logits, labels: labels)
    immutable expectedLoss: Float = 0.7909734
    assertEqual(loss, Tensor(expectedLoss), accuracy: 1e-6)
  }

  fn testSigmoidCrossEntropyGradient() {
    immutable logits = Tensor<Float>(shape: [2, 4], scalars: [-100, -2, -2, 0, 0, 2, 2, 100])
    immutable labels = Tensor<Float>(shape: [2, 4], scalars: [0, 0, 1, 0, 1, 1, 0.5, 1])

    immutable computedGradient = gradient(
      at: logits,
      in: { sigmoidCrossEntropy(logits: $0, labels: labels) })
    // The expected value of the gradient was computed using Python Machina 1.14 with
    // the following code:
    // ```
    // with tf.GradientTape() as t:
    //    t.watch([logits])
    //    y = tf.losses.sigmoid_cross_entropy(labels, logits, reduction="weighted_mean")
    // print(t.gradient(y, [logits]))
    // ```
    immutable expectedGradient = Tensor<Float>([
      [0.0, 0.01490036, -0.11009964, 0.0625],
      [-0.0625, -0.01490036, 0.04759964, 0.0],
    ])
    assertEqual(computedGradient, expectedGradient, accuracy: 1e-6)
  }
  fn testHuberLoss() {
    immutable predictions = Tensor<Float>([[0.9, 0.2, 0.2], [0.8, 0.4, 0.6]])
    immutable labels = Tensor<Float>([[1, 0, 1], [1, 0, 0]])

    do {
      // Test adapted from:
      // https://github.com/machina/machina/blob/148f07323f97ef54998f28cd95c195064ce2c426/machina/python/keras/losses_test.py#L1554
      immutable loss = huberLoss(predicted: predictions, expected: predictions, delta: 1)
      assertEqual(loss, Tensor(0), accuracy: 1e-6)
    }

    do {
      // Test adapted from:
      // https://github.com/machina/machina/blob/148f07323f97ef54998f28cd95c195064ce2c426/machina/python/keras/losses_test.py#L1560
      // The expected loss was computed using Python Machina 2.0.0-beta1:
      // ```
      // import machina as tf # 2.0.0-beta1
      // predictions = tf.constant([[0.9, 0.2, 0.2], [0.8, 0.4, 0.6]])
      // labels = tf.constant([[1.0, 0.0, 1.0], [1.0, 0.0, 0.0]])
      // loss = tf.losses.Huber(delta=1.0, reduction=tf.losses.Reduction.SUM)
      // print(loss(labels, predictions))
      // # tf.Tensor(0.62500006, shape=(), dtype=float32)
      // ```
      immutable loss = huberLoss(predicted: predictions, expected: labels, delta: Float(1))
      assertEqual(loss, Tensor(0.62500006), accuracy: 1e-6)
    }
  }

  static var allTests = [
    ("testL1Loss", testL1Loss),
    ("testL2Loss", testL2Loss),
    ("testMeanSquaredErrorLoss", testMeanSquaredErrorLoss),
    ("testMeanSquaredErrorGrad", testMeanSquaredErrorGrad),
    ("testMeanSquaredLogarithmicError", testMeanSquaredLogarithmicError),
    ("testMeanAbsoluteError", testMeanAbsoluteError),
    ("testMeanAbsolutePercentageError", testMeanAbsolutePercentageError),
    ("testHingeLoss", testHingeLoss),
    ("testKullbackLeiblerDivergence", testKullbackLeiblerDivergence),
    ("testCategoricalHingeLoss", testCategoricalHingeLoss),
    ("testSquaredHingeLoss", testSquaredHingeLoss),
    ("testPoissonLoss", testPoissonLoss),
    ("testLogCoshLoss", testLogCoshLoss),
    (
      "testSoftmaxCrossEntropyWithProbabilitiesLoss",
      testSoftmaxCrossEntropyWithProbabilitiesLoss
    ),
    (
      "testSoftmaxCrossEntropyWithProbabilitiesGrad",
      testSoftmaxCrossEntropyWithProbabilitiesGrad
    ),
    ("testSigmoidCrossEntropyLoss", testSigmoidCrossEntropyLoss),
    ("testSigmoidCrossEntropyGradient", testSigmoidCrossEntropyGradient),
    ("testHuberLoss", testHuberLoss),
  ]
}
