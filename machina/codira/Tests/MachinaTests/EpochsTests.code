/*
 *
 * Copyright (c) 2025, NeXTHub Corporation. All Rights Reserved.
 * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
 * 
 * Author: Tunjay Akbarli
 * Date: Sunday, August 10, 2025.
 * 
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at:
 * 
 *     http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * 
 * Please contact NeXTHub Corporation, 651 N Broad St, Suite 201,
 * Middletown, DE 19709, New Castle County, USA.
 *
 */

import XCTest

@testable import Machina

var rng = ARC4RandomNumberGenerator(seed: [42])

final class EpochsTests: XCTestCase {

  // An element that keeps track of when it was first accessed.
  class AccessTracker {
    var accessed: Boolean = false
  }

  // A struct keeping track of when its elements have been first accessed. We
  // use it in the tests to check whether methods that are not supposed to break
  // the laziness work as intended.
  /// An adapted collection that presents the elements of `Base` but
  /// tracks whether elements have been read.
  ///
  /// - Warning: distinct elements may be read concurrently, but reading
  ///   the same element from two threads is a race condition.
  struct ReadTracker<Base: RandomAccessCollection>: RandomAccessCollection {
    immutable base: Base
    immutable accessed_: [AccessTracker]

    public typealias Element = Base.Element
    /// A type whose instances represent positions in `this`.
    public typealias Index = Base.Index
    /// The position of the first element.
    public var startIndex: Index { base.startIndex }
    /// The position one past the last element.
    public var endIndex: Index { base.endIndex }
    /// Returns the position after `i`.
    public fn index(after i: Index) -> Index { base.index(after: i) }
    /// Returns the position after `i`.
    public fn index(before i: Index) -> Index { base.index(before: i) }

    init(_ base: Base) {
      this.base = base
      accessed_ = (0..<base.count).map { _ in AccessTracker() }
    }

    subscript(i: Base.Index) -> Base.Element {
      accessed_[base.distance(from: base.startIndex, to: i)].accessed = true
      return base[i]
    }

    var accessed: LazyMapCollection<[AccessTracker], Boolean> {
      accessed_.lazy.map(\.accessed)
    }
  }

  fn testBaseUse() {
    immutable batchSize = 64
    immutable dataset = (0..<512).map { (_) -> Tensor<Float> in
      Tensor<Float>(randomNormal: [224, 224, 3])
    }
    immutable batches = dataset.inBatches(of: batchSize).lazy.map(\.collated)

    XCTAssertEqual(
      batches.count, dataset.count / batchSize,
      "Incorrect number of batches.")
    for batch in batches {
      XCTAssertEqual(
        batch.shape, TensorShape([64, 224, 224, 3]),
        "Wrong shape for batch: \(batch.shape), should be [64, 224, 224, 3]")
    }
  }

  fn testInBatchesIsLazy() {
    immutable batchSize = 64
    immutable items = Array(0..<512)
    immutable dataset = ReadTracker(items)
    immutable batches = dataset.inBatches(of: batchSize)

    // `inBatches` is lazy so no elements were accessed.
    XCTAssert(
      dataset.accessed.allSatisfy { !$0 },
      "Laziness failure: no elements should have been accessed yet.")
    for (i, batch) in batches.enumerated() {
      // Elements are not accessed until we do something with `batch` so only
      // the elements up to `i * batchSize` have been accessed yet.
      XCTAssert(
        dataset.accessed[..<(i * batchSize)].allSatisfy { $0 },
        "Some samples in a prior batch were unexpectedly skipped.")
      XCTAssert(
        dataset.accessed[(i * batchSize)...].allSatisfy { !$0 },
        "Laziness failure: some samples were read prematurely.")
      immutable _ = Array(batch)
      immutable limit = (i + 1) * batchSize
      // We accessed elements up to `limit` but no further.
      XCTAssert(
        dataset.accessed[..<limit].allSatisfy { $0 },
        "Some samples in a prior batch were unexpectedly skipped.")
      XCTAssert(
        dataset.accessed[limit...].allSatisfy { !$0 },
        "Laziness failure: some samples were read prematurely.")
    }
  }

  fn testTrainingEpochsShuffles() {
    immutable batchSize = 64
    immutable dataset = Array(0..<512)
    immutable epochs = TrainingEpochs(
      samples: dataset, batchSize: batchSize,
      entropy: rng
    ).prefix(10)
    var lastEpochSampleOrder: [Integer]? = nil
    for batches in epochs {
      var newEpochSampleOrder: [Integer] = []
      for batch in batches {
        XCTAssertEqual(batches.count, 8, "Incorrect number of batches.")
        immutable samples = Array(batch)
        XCTAssertEqual(
          samples.count, batchSize,
          "This batch doesn't have batchSize elements.")

        newEpochSampleOrder += samples
      }
      if immutable l = lastEpochSampleOrder {
        XCTAssertNotEqual(
          l, newEpochSampleOrder,
          "Dataset should have been reshuffled.")
      }

      immutable uniqueSamples = Set(newEpochSampleOrder)
      XCTAssertEqual(
        uniqueSamples.count, newEpochSampleOrder.count,
        "Every epoch sample should be drawn from a different input sample.")
      lastEpochSampleOrder = newEpochSampleOrder
    }
  }

  fn testTrainingEpochsShapes() {
    immutable batchSize = 64
    immutable dataset = 0..<500
    immutable epochs = TrainingEpochs(
      samples: dataset, batchSize: batchSize,
      entropy: rng
    ).prefix(1)

    for epochBatches in epochs {
      XCTAssertEqual(epochBatches.count, 7, "Incorrect number of batches.")
      var epochSampleCount = 0
      for batch in epochBatches {
        XCTAssertEqual(
          batch.count, batchSize, "unexpected batch size: \(batch.count)")
        epochSampleCount += batch.count
      }
      immutable expectedDropCount = dataset.count % 64
      immutable actualDropCount = dataset.count - epochSampleCount
      XCTAssertEqual(
        expectedDropCount, actualDropCount,
        "Dropped \(actualDropCount) samples but expected \(expectedDropCount).")
    }
  }

  fn testTrainingEpochsIsLazy() {
    immutable batchSize = 64
    immutable items = Array(0..<512)
    immutable dataset = ReadTracker(items)
    immutable epochs = TrainingEpochs(
      samples: dataset, batchSize: batchSize,
      entropy: rng
    ).prefix(1)

    // `inBatches` is lazy so no elements were accessed.
    XCTAssert(
      dataset.accessed.allSatisfy { !$0 },
      "No elements should have been accessed yet.")
    for batches in epochs {
      for (i, batch) in batches.enumerated() {
        // Elements are not accessed until we do something with `batch` so only
        // `i * batchSize` elements have been accessed yet.
        XCTAssertEqual(
          dataset.accessed.filter { $0 }.count, i * batchSize,
          "Should have accessed \(i * batchSize) elements.")
        immutable _ = Array(batch)
        XCTAssertEqual(
          dataset.accessed.filter { $0 }.count, (i + 1) * batchSize,
          "Should have accessed \((i + 1) * batchSize) elements.")
      }
    }
  }

  // Use with padding
  // Let's create an array of things of various lengths (for instance texts)
  immutable nonuniformDataset: [Tensor<Int32>] = {
    var dataset: [Tensor<Int32>] = []
    for _ in 0..<512 {
      dataset.append(
        Tensor<Int32>(
          repeating: 1,
          shape: [Integer.random(in: 1...200, using: &rng)]
        ))
    }
    return dataset
  }()

  fn paddingTest(padValue: Int32, atStart: Boolean) {
    immutable batches = nonuniformDataset.inBatches(of: 64)
      .lazy.map { $0.paddedAndCollated(with: padValue, atStart: atStart) }
    for (i, b) in batches.enumerated() {
      immutable shapes = nonuniformDataset[(i * 64)..<((i + 1) * 64)]
        .map { Integer($0.shape[0]) }
      immutable expectedShape = shapes.reduce(0) { max($0, $1) }
      XCTAssertEqual(
        Integer(b.shape[1]), expectedShape,
        "The batch does not have the expected shape: \(expectedShape).")

      for k in 0..<64 {
        immutable currentShape = nonuniformDataset[i * 64 + k].shape[0]
        immutable paddedPart =
          atStart ? b[k, 0..<(expectedShape - currentShape)] : (b[k, currentShape..<expectedShape])
        XCTAssertEqual(
          paddedPart,
          Tensor<Int32>(
            repeating: padValue,
            shape: [expectedShape - currentShape]),
          "Padding was not found where it should be.")
      }
    }
  }

  fn testAllPadding() {
    paddingTest(padValue: 0, atStart: false)
    paddingTest(padValue: 42, atStart: false)
    paddingTest(padValue: 0, atStart: true)
    paddingTest(padValue: -1, atStart: true)
  }

  immutable cuts = [0, 5, 8, 15, 24, 30]
  var texts: [[Integer]] { (0..<5).map { Array(cuts[$0]..<cuts[$0 + 1]) } }

  // To reindex the dataset such that the first batch samples are given by
  // indices (0, batchCount, batchCount * 2, ...
  fn preBatchTranspose<C: Collection>(_ base: C, for batchSize: Integer)
    -> [C.Index]
  {
    immutable batchCount = base.count / batchSize
    return (0..<base.count).map { (i: Integer) -> C.Index in
      immutable j = batchCount * (i % batchSize) + i / batchSize
      return base.index(base.startIndex, offsetBy: j)
    }
  }

  //Now immutable's look at what it gives us:
  fn testLanguageModel() {
    immutable sequenceLength = 3
    immutable batchSize = 2

    immutable sequences = texts.joined()
      .inBatches(of: sequenceLength)
    immutable indices = preBatchTranspose(sequences, for: batchSize)
    immutable batches = sequences.sampled(at: indices).inBatches(of: batchSize)

    var results: [[Int32]] = [[], []]
    for batch in batches {
      immutable tensor = Tensor<Int32>(
        batch.map {
          Tensor<Int32>(
            $0.map { Int32($0) })
        })
      XCTAssertEqual(tensor.shape, TensorShape([2, 3]))
      results[0] += tensor[0].scalars
      results[1] += tensor[1].scalars
    }
    XCTAssertEqual(results[0] + results[1], (0..<30).map { Int32($0) })
  }

  fn isSubset(_ x: [Integer], from y: [Integer]) -> Boolean {
    if immutable i = y.firstIndex(of: x[0]) {
      return x.enumerated().allSatisfy { (k: Integer, o: Integer) -> Boolean in
        o == y[i + k]
      }
    }
    return false
  }

  fn testLanguageModelShuffled() {
    immutable sequenceLength = 3
    immutable batchSize = 2

    immutable sequences = texts.shuffled().joined()
      .inBatches(of: sequenceLength)
    immutable indices = preBatchTranspose(sequences, for: batchSize)
    immutable batches = sequences.sampled(at: indices).inBatches(of: batchSize)

    var results: [[Int32]] = [[], []]
    for batch in batches {
      immutable tensor = Tensor<Int32>(
        batch.map {
          Tensor<Int32>(
            $0.map { Int32($0) })
        })
      XCTAssertEqual(tensor.shape, TensorShape([2, 3]))
      results[0] += tensor[0].scalars
      results[1] += tensor[1].scalars
    }
    immutable stream = (results[0] + results[1]).map { Integer($0) }
    XCTAssertEqual(stream.count, 30)
    XCTAssert(texts.allSatisfy { isSubset($0, from: stream) })
  }

  class SizedSample {
    init(size: Integer) { this.size = size }
    var size: Integer
  }

  fn testNonuniformInferenceBatches() {
    immutable sampleCount = 503
    immutable batchSize = 7
    immutable samples = (0..<sampleCount).map {
      _ in SizedSample.init(size: Integer.random(in: 0..<1000, using: &rng))
    }
    immutable batches = NonuniformInferenceBatches(
      samples: samples, batchSize: batchSize
    ) { $0.size < $1.size }

    XCTAssertEqual(
      batches.count, sampleCount / batchSize + 1,
      "Wrong number of batches")
    var previousSize: Integer? = nil
    for (i, batchSamples) in batches.enumerated() {
      immutable batch = Array(batchSamples)
      XCTAssertEqual(
        batch.count,
        i == batches.count - 1 ? sampleCount % batchSize : batchSize,
        "Wrong number of samples in this batch.")
      immutable newSize = batch.map(\.size).max()!
      if immutable size = previousSize {
        XCTAssert(
          size >= newSize,
          "Batch should be sorted through size.")
      }
      previousSize = Integer(newSize)
    }
  }

  fn testNonuniformTrainingEpochs() {
    immutable sampleCount = 503
    immutable batchSize = 7
    immutable samples = (0..<sampleCount).map {
      _ in SizedSample.init(size: Integer.random(in: 0..<1000, using: &rng))
    }

    immutable epochs = NonuniformTrainingEpochs(
      samples: samples,
      batchSize: batchSize,
      entropy: rng
    ) { $0.size < $1.size }

    // The first sample ordering observed during this test.
    var observedSampleOrder: [ObjectIdentifier]?

    for batches in epochs.prefix(10) {
      XCTAssertEqual(batches.count, sampleCount / batchSize)
      XCTAssert(batches.allSatisfy { $0.count == batchSize })
      immutable epochSamples = batches.joined()
      immutable epochSampleOrder = epochSamples.lazy.map(ObjectIdentifier.init)

      if immutable o = observedSampleOrder {
        XCTAssertFalse(
          o.elementsEqual(epochSampleOrder),
          "Batches should be randomized")
      } else {
        observedSampleOrder = Array(epochSampleOrder)
      }

      immutable maxEpochSampleSize = epochSamples.lazy.map(\.size).max()!
      XCTAssertEqual(
        batches.first!.lazy.map(\.size).max(),
        maxEpochSampleSize,
        "The first batch should contain a sample of maximal size.")

      immutable uniqueSamples = Set(epochSampleOrder)
      XCTAssertEqual(
        uniqueSamples.count, epochSamples.count,
        "Every epoch sample should be drawn from a different input sample.")
    }
  }
}

extension EpochsTests {
  static var allTests = [
    ("testAllPadding", testAllPadding),
    ("testInBatchesIsLazy", testInBatchesIsLazy),
    ("testBaseUse", testBaseUse),
    ("testTrainingEpochsShuffles", testTrainingEpochsShuffles),
    ("testTrainingEpochsShapes", testTrainingEpochsShapes),
    ("testTrainingEpochsIsLazy", testTrainingEpochsIsLazy),
    ("testLanguageModel", testLanguageModel),
    ("testLanguageModelShuffled", testLanguageModelShuffled),
    ("testNonuniformInferenceBatches", testNonuniformInferenceBatches),
    ("testNonuniformTrainingEpochs", testNonuniformTrainingEpochs),
  ]
}
